{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice of Overall Classifier Architecture:\n",
    "From researching the topic, our group was initially unable to form a definitive conclusion since most answers seemed to (vaguely) point that the best approach would depend on the nature of the problem, the amount of data available, and computation power.\n",
    "\n",
    "As recommended by Prof Mari, we decided to build out both architectures with a relatively simple model as an experiment. Even though the multiclass classifier achieved a higher overall accuracy, we then realised that it would be far easier to diagnose and tweak two binary classifiers separately than in a single model, and hence went with the cascaded structure instead.\n",
    "\n",
    "Our rationale for this choice is that our overall goal would be to build a model with high sensitivity (rather than accuracy) in order to pick out infected cases from the data and ensure that these patients can seek medical treatment - favouring Type 1 over Type 2 errors. Additionally, since Covid-19 is highly contagious, there is an impetus to try detect as many of these from the infected cases as well. Hence, the cascaded binary classifier structure naturally aligns with this by allowing us to finetune for sensitivity at each stage.\n",
    "\n",
    "Below is a brief summary of what we believe to be the tradeoffs of each model.\n",
    "\n",
    "## Comparison Summary \n",
    "#### Multiclass Pros\n",
    "- Simple to use since we only have to deal with a single model\n",
    "- Seemed to be able to obtain higher overall accuracy than the cascaded structure on this particular dataset (85% versus 90%*75%=~70%)\n",
    "\n",
    "#### Multiclass Cons\n",
    "- Hard to diagnose, and thus difficult to finetune the relationship between the three classes\n",
    "\n",
    "#### Cascaded Binary Pros\n",
    "- Easy to input weights for different classifications in the step-wise process to finetune sensitivity\n",
    "- Converges faster on a per-model basis\n",
    "- More accurate in theory due to the potential of modeling pair-wise relationships between classes\n",
    "\n",
    "#### Cascaded Binary Cons\n",
    "- Irritating to tweak and evaluate, in addition to needing extra steps to build the dataloaders for each one\n",
    "- Overall time to train is longer due to the presence of two models\n",
    "\n",
    "\n",
    "Logically speaking, this may be why doctors in practice do not use x-ray scans to classify the source of lung damage, but only to confirm the presence of it. Hence, this exercise of actually determining what caused the lung damage is a fallacy in itself, and the best way to account for this is to use a separate classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice of CNN Architecture\n",
    "Rather than conceive a completely new architecture from our limited knowledge, our group adopted an approach where we trained and evaluated a few of the available predefined models in torchvision, then selected the best contender to build from scratch so that we could tweak it.\n",
    "\n",
    "Given the constraints of 1) a relatively small training dataset and 2) limited computation power, we decided to keep the number of parameters on the lower side to follow the golden rule of machine learning. Hence, from the illustration below plotting top-1 accuracy against the number of operations, we selected the ResNet, DenseNet, MobileNet, and Inception architectures. Specifically, we chose ResNet-18, DenseNet-121, MobileNetV2 and Inception-v3.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/4000/1*n16lj3lSkz2miMc_5cvkrA.jpeg\" style=\"height:300px\">\n",
    "\n",
    "After testing the models, we found that ResNet-18 was the clear winner for our base model, consistently achieving the highest accuracy on the validation set with the lowest loss on the training set, while also taking the shortest time to train. The figures below show the best models achieved within 10 epochs for both binary classifiers #1 and #2.\n",
    "\n",
    "|                    | ResNet-18 | DenseNet121 | Inception-v3 | MobileNetV2 |\n",
    "|--------------------|-----------|-------------|--------------|-------------|\n",
    "| Train Avg. Loss #1 | 0.2689    | 0.2829      | 0.4727       | 0.4377      |\n",
    "| Val Avg. Loss #1   | 0.0288    | 0.0534      | 0.1573       | 0.1125      |\n",
    "| Val Accuracy #1    | 92%       | 88%         | 71%          | 83%         |\n",
    "| Training Time #1   | 6min 32s  | 21min 58s   | 16min 25s    | 8min 49s    |\n",
    "| Train Avg. Loss #2 | 0.5802    | 0.5992      | 0.7481       | 0.6383      |\n",
    "| Val Avg. Loss #2   | 0.1159    | 0.1134      | 0.1631       | 0.1508      |\n",
    "| Val Accuracy #2    | 75%       | 75%         | 56%          | 62%         |\n",
    "| Training Time #2   | 4min 49s  | 16min       | 12min        | 6min 32s    |\n",
    "\n",
    "\n",
    "### Observations \n",
    "- DenseNet also performed well, but required significantly more time to train than ResNet - more than 3x\n",
    "- Inception showed subpar performance (due to the removal of the aux channel), but still required significantly more time to train than ResNet.\n",
    "- MobileNet (our favoured contender) unfortunately obtained similar results to Inception, and somehow also took slightly longer to train\n",
    "- All the architectures struggled with the second task of separating covid and non-covid cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from train import train, validate\n",
    "\n",
    "resnet = models.resnet18(pretrained=False)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "resnet.fc = nn.Linear(num_ftrs, 2)\n",
    "# print(resnet)\n",
    "\n",
    "densenet = models.densenet121(pretrained=False)\n",
    "num_ftrs = densenet.classifier.in_features\n",
    "densenet.features.conv0 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "densenet.classifier = nn.Linear(num_ftrs, 2)\n",
    "# print(densenet)\n",
    "\n",
    "inception = models.inception_v3(pretrained=False, aux_logits=False) #disable auxiliary channel to accept 150x150 images\n",
    "inception.Conv2d_1a_3x3.conv=nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "num_ftrs = inception.fc.in_features\n",
    "inception.fc = nn.Linear(num_ftrs,2)\n",
    "# print(inception)\n",
    "\n",
    "mobilenet = models.mobilenet_v2(pretrained=False)\n",
    "mobilenet.features[0][0]=nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "num_ftrs = mobilenet.classifier[1].in_features\n",
    "mobilenet.classifier[1] = nn.Linear(num_ftrs, 2)\n",
    "# print(mobilenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\n",
      "Train set: Average loss: 0.5032\n",
      "Validation set: Average loss: 0.1801, Accuracy: 15/24 (62%)\n",
      "\n",
      "Found New Minima at epoch 1 loss: 0.18013657381137213\n",
      "\n",
      "Train Epoch: 2\n",
      "Train set: Average loss: 0.3512\n",
      "Validation set: Average loss: 0.1321, Accuracy: 19/24 (79%)\n",
      "\n",
      "Found New Minima at epoch 2 loss: 0.13212250793973604\n",
      "\n",
      "Train Epoch: 3\n",
      "Train set: Average loss: 0.3304\n",
      "Validation set: Average loss: 0.0604, Accuracy: 22/24 (92%)\n",
      "\n",
      "Found New Minima at epoch 3 loss: 0.06036650544653336\n",
      "\n",
      "Train Epoch: 4\n",
      "Train set: Average loss: 0.2968\n",
      "Validation set: Average loss: 0.1091, Accuracy: 20/24 (83%)\n",
      "\n",
      "Train Epoch: 5\n",
      "Train set: Average loss: 0.3056\n",
      "Validation set: Average loss: 0.0572, Accuracy: 23/24 (96%)\n",
      "\n",
      "Found New Minima at epoch 5 loss: 0.05722993488113085\n",
      "\n",
      "Train Epoch: 6\n",
      "Train set: Average loss: 0.3002\n",
      "Validation set: Average loss: 0.0506, Accuracy: 22/24 (92%)\n",
      "\n",
      "Found New Minima at epoch 6 loss: 0.05057169760887822\n",
      "\n",
      "Train Epoch: 7\n",
      "Train set: Average loss: 0.2689\n",
      "Validation set: Average loss: 0.0288, Accuracy: 22/24 (92%)\n",
      "\n",
      "Found New Minima at epoch 7 loss: 0.028785567575444777\n",
      "\n",
      "Train Epoch: 8\n",
      "Train set: Average loss: 0.2606\n",
      "Validation set: Average loss: 0.0490, Accuracy: 22/24 (92%)\n",
      "\n",
      "Train Epoch: 9\n",
      "Train set: Average loss: 0.2455\n",
      "Validation set: Average loss: 0.0670, Accuracy: 22/24 (92%)\n",
      "\n",
      "Train Epoch: 10\n",
      "Train set: Average loss: 0.2421\n",
      "Validation set: Average loss: 0.0510, Accuracy: 23/24 (96%)\n",
      "\n",
      "Time Elapsed: 0:06:32.241182\n",
      "\n",
      "Train Epoch: 1\n",
      "Train set: Average loss: 1.1077\n",
      "Validation set: Average loss: 0.1547, Accuracy: 12/16 (75%)\n",
      "\n",
      "Found New Minima at epoch 1 loss: 0.15469163842499256\n",
      "\n",
      "Train Epoch: 2\n",
      "Train set: Average loss: 0.6431\n",
      "Validation set: Average loss: 0.1623, Accuracy: 11/16 (69%)\n",
      "\n",
      "Train Epoch: 3\n",
      "Train set: Average loss: 0.6280\n",
      "Validation set: Average loss: 0.1389, Accuracy: 12/16 (75%)\n",
      "\n",
      "Found New Minima at epoch 3 loss: 0.13887844793498516\n",
      "\n",
      "Train Epoch: 4\n",
      "Train set: Average loss: 0.5990\n",
      "Validation set: Average loss: 0.1309, Accuracy: 13/16 (81%)\n",
      "\n",
      "Found New Minima at epoch 4 loss: 0.13087763264775276\n",
      "\n",
      "Train Epoch: 5\n",
      "Train set: Average loss: 0.5909\n",
      "Validation set: Average loss: 0.1481, Accuracy: 12/16 (75%)\n",
      "\n",
      "Train Epoch: 6\n",
      "Train set: Average loss: 0.5835\n",
      "Validation set: Average loss: 0.1207, Accuracy: 13/16 (81%)\n",
      "\n",
      "Found New Minima at epoch 6 loss: 0.1207497101277113\n",
      "\n",
      "Train Epoch: 7\n",
      "Train set: Average loss: 0.5792\n",
      "Validation set: Average loss: 0.1421, Accuracy: 13/16 (81%)\n",
      "\n",
      "Train Epoch: 8\n",
      "Train set: Average loss: 0.5802\n",
      "Validation set: Average loss: 0.1159, Accuracy: 12/16 (75%)\n",
      "\n",
      "Found New Minima at epoch 8 loss: 0.115862637758255\n",
      "\n",
      "Train Epoch: 9\n",
      "Train set: Average loss: 0.5792\n",
      "Validation set: Average loss: 0.1536, Accuracy: 11/16 (69%)\n",
      "\n",
      "Train Epoch: 10\n",
      "Train set: Average loss: 0.5776\n",
      "Validation set: Average loss: 0.1228, Accuracy: 12/16 (75%)\n",
      "\n",
      "Time Elapsed: 0:04:49.957193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from train import train_binary_normal_clf, train_binary_covid_clf\n",
    "\n",
    "train_binary_normal_clf(10, 4, savePath=None, model=resnet, weight=None, quiet=True)\n",
    "train_binary_covid_clf(10, 4, savePath=None, model=resnet, weight=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\n",
      "Train set: Average loss: 0.4614\n",
      "Validation set: Average loss: 0.1324, Accuracy: 16/24 (67%)\n",
      "\n",
      "Found New Minima at epoch 1 loss: 0.13236750600238642\n",
      "\n",
      "Train Epoch: 2\n",
      "Train set: Average loss: 0.3546\n",
      "Validation set: Average loss: 0.1055, Accuracy: 19/24 (79%)\n",
      "\n",
      "Found New Minima at epoch 2 loss: 0.10549515672028065\n",
      "\n",
      "Train Epoch: 3\n",
      "Train set: Average loss: 0.3266\n",
      "Validation set: Average loss: 0.1249, Accuracy: 19/24 (79%)\n",
      "\n",
      "Train Epoch: 4\n",
      "Train set: Average loss: 0.3046\n",
      "Validation set: Average loss: 0.0852, Accuracy: 20/24 (83%)\n",
      "\n",
      "Found New Minima at epoch 4 loss: 0.08515533133565138\n",
      "\n",
      "Train Epoch: 5\n",
      "Train set: Average loss: 0.2984\n",
      "Validation set: Average loss: 0.0933, Accuracy: 20/24 (83%)\n",
      "\n",
      "Train Epoch: 6\n",
      "Train set: Average loss: 0.2980\n",
      "Validation set: Average loss: 0.0687, Accuracy: 21/24 (88%)\n",
      "\n",
      "Found New Minima at epoch 6 loss: 0.06870392366545275\n",
      "\n",
      "Train Epoch: 7\n",
      "Train set: Average loss: 0.2829\n",
      "Validation set: Average loss: 0.0534, Accuracy: 21/24 (88%)\n",
      "\n",
      "Found New Minima at epoch 7 loss: 0.053382359289874635\n",
      "\n",
      "Train Epoch: 8\n",
      "Train set: Average loss: 0.2707\n",
      "Validation set: Average loss: 0.0651, Accuracy: 20/24 (83%)\n",
      "\n",
      "Train Epoch: 9\n",
      "Train set: Average loss: 0.2736\n",
      "Validation set: Average loss: 0.0701, Accuracy: 20/24 (83%)\n",
      "\n",
      "Train Epoch: 10\n",
      "Train set: Average loss: 0.2550\n",
      "Validation set: Average loss: 0.0638, Accuracy: 22/24 (92%)\n",
      "\n",
      "Time Elapsed: 0:21:58.655482\n",
      "\n",
      "Train Epoch: 1\n",
      "Train set: Average loss: 1.5462\n",
      "Validation set: Average loss: 0.2615, Accuracy: 7/16 (44%)\n",
      "\n",
      "Found New Minima at epoch 1 loss: 0.2614750377833843\n",
      "\n",
      "Train Epoch: 2\n",
      "Train set: Average loss: 0.7773\n",
      "Validation set: Average loss: 0.1341, Accuracy: 12/16 (75%)\n",
      "\n",
      "Found New Minima at epoch 2 loss: 0.13412632420659065\n",
      "\n",
      "Train Epoch: 3\n",
      "Train set: Average loss: 0.6346\n",
      "Validation set: Average loss: 0.1205, Accuracy: 13/16 (81%)\n",
      "\n",
      "Found New Minima at epoch 3 loss: 0.12049849331378937\n",
      "\n",
      "Train Epoch: 4\n",
      "Train set: Average loss: 0.6065\n",
      "Validation set: Average loss: 0.1334, Accuracy: 12/16 (75%)\n",
      "\n",
      "Train Epoch: 5\n",
      "Train set: Average loss: 0.6046\n",
      "Validation set: Average loss: 0.1284, Accuracy: 12/16 (75%)\n",
      "\n",
      "Train Epoch: 6\n",
      "Train set: Average loss: 0.5949\n",
      "Validation set: Average loss: 0.1461, Accuracy: 11/16 (69%)\n",
      "\n",
      "Train Epoch: 7\n",
      "Train set: Average loss: 0.5992\n",
      "Validation set: Average loss: 0.1134, Accuracy: 12/16 (75%)\n",
      "\n",
      "Found New Minima at epoch 7 loss: 0.1134311705827713\n",
      "\n",
      "Train Epoch: 8\n",
      "Train set: Average loss: 0.5836\n",
      "Validation set: Average loss: 0.1228, Accuracy: 11/16 (69%)\n",
      "\n",
      "Train Epoch: 9\n",
      "Train set: Average loss: 0.5878\n",
      "Validation set: Average loss: 0.1403, Accuracy: 11/16 (69%)\n",
      "\n",
      "Train Epoch: 10\n",
      "Train set: Average loss: 0.5765\n",
      "Validation set: Average loss: 0.1553, Accuracy: 12/16 (75%)\n",
      "\n",
      "Time Elapsed: 0:16:19.505228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_binary_normal_clf(10, 4, savePath=None, model=densenet, weight=None, quiet=True)\n",
    "train_binary_covid_clf(10, 4, savePath=None, model=densenet, weight=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\n",
      "Train set: Average loss: 0.5331\n",
      "Validation set: Average loss: 0.1753, Accuracy: 14/24 (58%)\n",
      "\n",
      "Found New Minima at epoch 1 loss: 0.17527922677497068\n",
      "\n",
      "Train Epoch: 2\n",
      "Train set: Average loss: 0.4877\n",
      "Validation set: Average loss: 0.1918, Accuracy: 16/24 (67%)\n",
      "\n",
      "Train Epoch: 3\n",
      "Train set: Average loss: 0.5236\n",
      "Validation set: Average loss: 0.2788, Accuracy: 14/24 (58%)\n",
      "\n",
      "Train Epoch: 4\n",
      "Train set: Average loss: 0.5581\n",
      "Validation set: Average loss: 0.2099, Accuracy: 16/24 (67%)\n",
      "\n",
      "Train Epoch: 5\n",
      "Train set: Average loss: 0.5609\n",
      "Validation set: Average loss: 0.2309, Accuracy: 16/24 (67%)\n",
      "\n",
      "Train Epoch: 6\n",
      "Train set: Average loss: 0.5106\n",
      "Validation set: Average loss: 0.2347, Accuracy: 15/24 (62%)\n",
      "\n",
      "Train Epoch: 7\n",
      "Train set: Average loss: 0.4727\n",
      "Validation set: Average loss: 0.1573, Accuracy: 17/24 (71%)\n",
      "\n",
      "Found New Minima at epoch 7 loss: 0.15726305293113305\n",
      "\n",
      "Train Epoch: 8\n",
      "Train set: Average loss: 0.4305\n",
      "Validation set: Average loss: 0.2191, Accuracy: 16/24 (67%)\n",
      "\n",
      "Train Epoch: 9\n",
      "Train set: Average loss: 0.4347\n",
      "Validation set: Average loss: 0.3563, Accuracy: 16/24 (67%)\n",
      "\n",
      "Train Epoch: 10\n",
      "Train set: Average loss: 0.4249\n",
      "Validation set: Average loss: 0.2504, Accuracy: 16/24 (67%)\n",
      "\n",
      "Time Elapsed: 0:16:25.828651\n",
      "\n",
      "Train Epoch: 1\n",
      "Train set: Average loss: 2.5412\n",
      "Validation set: Average loss: 0.5180, Accuracy: 6/16 (38%)\n",
      "\n",
      "Found New Minima at epoch 1 loss: 0.5179687812924385\n",
      "\n",
      "Train Epoch: 2\n",
      "Train set: Average loss: 1.0828\n",
      "Validation set: Average loss: 0.4637, Accuracy: 9/16 (56%)\n",
      "\n",
      "Found New Minima at epoch 2 loss: 0.4637055266648531\n",
      "\n",
      "Train Epoch: 3\n",
      "Train set: Average loss: 0.7481\n",
      "Validation set: Average loss: 0.1631, Accuracy: 9/16 (56%)\n",
      "\n",
      "Found New Minima at epoch 3 loss: 0.16309987008571625\n",
      "\n",
      "Train Epoch: 4\n",
      "Train set: Average loss: 0.7087\n",
      "Validation set: Average loss: 0.1765, Accuracy: 9/16 (56%)\n",
      "\n",
      "Train Epoch: 5\n",
      "Train set: Average loss: 0.7174\n",
      "Validation set: Average loss: 0.1935, Accuracy: 9/16 (56%)\n",
      "\n",
      "Train Epoch: 6\n",
      "Train set: Average loss: 0.7033\n",
      "Validation set: Average loss: 0.1978, Accuracy: 9/16 (56%)\n",
      "\n",
      "Train Epoch: 7\n",
      "Train set: Average loss: 0.6855\n",
      "Validation set: Average loss: 0.1901, Accuracy: 9/16 (56%)\n",
      "\n",
      "Train Epoch: 8\n",
      "Train set: Average loss: 0.7025\n",
      "Validation set: Average loss: 0.1853, Accuracy: 10/16 (62%)\n",
      "\n",
      "Train Epoch: 9\n",
      "Train set: Average loss: 0.6906\n",
      "Validation set: Average loss: 0.1689, Accuracy: 9/16 (56%)\n",
      "\n",
      "Train Epoch: 10\n",
      "Train set: Average loss: 0.6951\n",
      "Validation set: Average loss: 0.1936, Accuracy: 9/16 (56%)\n",
      "\n",
      "Time Elapsed: 0:12:12.456683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_binary_normal_clf(10, 4, savePath=None, model=inception, weight=None, quiet=True)\n",
    "train_binary_covid_clf(10, 4, savePath=None, model=inception, weight=None, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1\n",
      "Train set: Average loss: 0.5432\n",
      "Validation set: Average loss: 0.1180, Accuracy: 16/24 (67%)\n",
      "\n",
      "Found New Minima at epoch 1 loss: 0.11798713852961858\n",
      "\n",
      "Train Epoch: 2\n",
      "Train set: Average loss: 0.4377\n",
      "Validation set: Average loss: 0.1125, Accuracy: 20/24 (83%)\n",
      "\n",
      "Found New Minima at epoch 2 loss: 0.11252926041682561\n",
      "\n",
      "Train Epoch: 3\n",
      "Train set: Average loss: 0.4176\n",
      "Validation set: Average loss: 0.1160, Accuracy: 17/24 (71%)\n",
      "\n",
      "Train Epoch: 4\n",
      "Train set: Average loss: 0.4309\n",
      "Validation set: Average loss: 0.1445, Accuracy: 18/24 (75%)\n",
      "\n",
      "Train Epoch: 5\n",
      "Train set: Average loss: 0.4226\n",
      "Validation set: Average loss: 0.1505, Accuracy: 17/24 (71%)\n",
      "\n",
      "Train Epoch: 6\n",
      "Train set: Average loss: 0.4203\n",
      "Validation set: Average loss: 0.1665, Accuracy: 18/24 (75%)\n",
      "\n",
      "Train Epoch: 7\n",
      "Train set: Average loss: 0.4395\n",
      "Validation set: Average loss: 0.1420, Accuracy: 18/24 (75%)\n",
      "\n",
      "Train Epoch: 8\n",
      "Train set: Average loss: 0.4257\n",
      "Validation set: Average loss: 0.1222, Accuracy: 17/24 (71%)\n",
      "\n",
      "Train Epoch: 9\n",
      "Train set: Average loss: 0.4033\n",
      "Validation set: Average loss: 0.1789, Accuracy: 17/24 (71%)\n",
      "\n",
      "Train Epoch: 10\n",
      "Train set: Average loss: 0.4364\n",
      "Validation set: Average loss: 0.1826, Accuracy: 15/24 (62%)\n",
      "\n",
      "Time Elapsed: 0:08:49.410031\n",
      "\n",
      "Train Epoch: 1\n",
      "Train set: Average loss: 1.3180\n",
      "Validation set: Average loss: 0.1964, Accuracy: 8/16 (50%)\n",
      "\n",
      "Found New Minima at epoch 1 loss: 0.19638646766543388\n",
      "\n",
      "Train Epoch: 2\n",
      "Train set: Average loss: 0.6878\n",
      "Validation set: Average loss: 0.1789, Accuracy: 6/16 (38%)\n",
      "\n",
      "Found New Minima at epoch 2 loss: 0.17894272133708\n",
      "\n",
      "Train Epoch: 3\n",
      "Train set: Average loss: 0.6449\n",
      "Validation set: Average loss: 0.1835, Accuracy: 8/16 (50%)\n",
      "\n",
      "Train Epoch: 4\n",
      "Train set: Average loss: 0.6416\n",
      "Validation set: Average loss: 0.1837, Accuracy: 9/16 (56%)\n",
      "\n",
      "Train Epoch: 5\n",
      "Train set: Average loss: 0.6416\n",
      "Validation set: Average loss: 0.1848, Accuracy: 8/16 (50%)\n",
      "\n",
      "Train Epoch: 6\n",
      "Train set: Average loss: 0.6404\n",
      "Validation set: Average loss: 0.1621, Accuracy: 10/16 (62%)\n",
      "\n",
      "Found New Minima at epoch 6 loss: 0.1621499713510275\n",
      "\n",
      "Train Epoch: 7\n",
      "Train set: Average loss: 0.6383\n",
      "Validation set: Average loss: 0.1508, Accuracy: 10/16 (62%)\n",
      "\n",
      "Found New Minima at epoch 7 loss: 0.1507614701986313\n",
      "\n",
      "Train Epoch: 8\n",
      "Train set: Average loss: 0.6356\n",
      "Validation set: Average loss: 0.1668, Accuracy: 8/16 (50%)\n",
      "\n",
      "Train Epoch: 9\n",
      "Train set: Average loss: 0.6370\n",
      "Validation set: Average loss: 0.1798, Accuracy: 8/16 (50%)\n",
      "\n",
      "Train Epoch: 10\n",
      "Train set: Average loss: 0.6397\n",
      "Validation set: Average loss: 0.1697, Accuracy: 9/16 (56%)\n",
      "\n",
      "Time Elapsed: 0:06:32.558262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_binary_normal_clf(10, 4, savePath=None, model=mobilenet, weight=None, quiet=True)\n",
    "train_binary_covid_clf(10, 4, savePath=None, model=mobilenet, weight=None, quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Modified ResNet:\n",
    "The final model we used is a shortened 10 layer version of ResNet-18. The rationale for this is that the input images to this model (150 x 150) are smaller than the typical images used in ResNet (224 x 224), and hence have less features that need to be learned. Additionally, the full ResNet-18 showed overfitting after longer training, and we believed decreasing the layers would allow the model to generalise better.\n",
    "\n",
    "We also experimented with adding dropout layers, but only saw decreases in overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (layers): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): ResBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (6): ResBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): ResBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (9): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from model import ResNet\n",
    "model=ResNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of loss function and optimizer\n",
    "We used the multiclass cross entropy loss criterion implemented in pytorch for convenience, which combines LogSoftmax and NLLLoss in one single class.\n",
    "\n",
    "In order to finetune the sensitivity of our model at each stage, we input a weight tensor which would penalise incorrect 'positive' classifications more (infected and covid for stage 1 and 2 respectively). While perhaps slightly 'hacky', it was successful in increasing the sensitivity. Through some trial and error, we arrived at weights [1.0, 3.77] and [1.0, 2.8896] which maximised sensitivity without sacrificing absurd amounts of accuracy.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
